
# en_math_2022_07-

# en_math_2022_07-13

## 《Interactive Linear Algebra》_1.2 Row Reduction

### Objectives
1. Learn to replace a system of linear equations by an augmented matrix.  
2. Learn how the elimination method corresponds to performing row operations on an augmented matrix.  
3. Understand when a matrix is in (reduced) row echelon form.  
4. Learn which row reduced matrices come from inconsistent linear systems.  
5. Recipe: the row reduction algorithm.  
6. Vocabulary words:  
    * row operation,  
    * row equivalence,  
    * matrix,  
    * augmented matrix,  
    * pivot,  
    * (reduced) row echelon form.  

In this section, we will present an algorithm for “solving” a system of linear equations.

### The Elimination Method

We will solve systems of linear equations algebraically using the elimination method.  
In other words, we will combine the equations in various ways to try to eliminate as many variables as possible from each equation.  
There are three valid operations we can perform on our system of equations:

* **Scaling**:  
we can multiply both sides of an equation by a nonzero number.

$$\begin{cases} x+2y+3z=6 \\\ 2x+3y+2z=14 \\\ 3x+y-z=-2 \end{cases} \underrightarrow{\ multiply \  1st \  by \ −3} \begin{cases} -3x-6y-9z=-18 \\\ 2x+3y+2z=14 \\\ 3x+y-z=-2 \end{cases}$$

* **Replacement**:  
we can add a multiple of one equation to another, replacing the second equation with the result.

$$\begin{cases} x+2y+3z=6 \\\ 2x+3y+2z=14 \\\ 3x+y-z=-2 \end{cases} \underrightarrow{\ 2nd=2nd- 2\times 1st} \begin{cases} x+2y+3z=6 \\\ -7y-4z=2 \\\ 3x+y-z=-2 \end{cases}$$

* **Swap**:  
we can swap two equations.
$$\begin{cases} x+2y+3z=6 \\\ 2x+3y+2z=14 \\\ 3x+y-z=-2 \end{cases} \underrightarrow{\ 3rd \longleftrightarrow 1st} \begin{cases} 3x+y-z=-2 \\\ -7y-4z=2 \\\ x+2y+3z=6 \end{cases}$$

>Example
Solve (1.1.1) using the elimination method.

showTable                                                             | no
----------------------------------------------------------------------|--------
$\begin{cases} x+2y+3z=6 \\\ 2x-3y+2z=14 \\\ 3x+y-z=-2 \end{cases}$   | (1.1.1)

>Solution  

$\begin{cases} x+2y+3z=6 \\\ 2x-3y+2z=14 \\\ 3x+y-z=-2 \end{cases} $

$\underrightarrow{2nd \ = \ 2nd-2 \times 1st} \begin{cases} x+2y+3z=6 \\\ -7y-4z=2 \\\ 3x+y-z=-2 \end{cases}$

$\underrightarrow{3rd \ = \ 3rd-3 \times 1st} \begin{cases} x+2y+3z=6 \\\ -7y-4z=2 \\\ -5y-10z=-20 \end{cases}$

$\underrightarrow{\ 2nd \longleftrightarrow 3rd} \begin{cases} x+2y+3z=6 \\\ -5y-10z=-20 \\\ -7y-4z=2 \end{cases}$

$\underrightarrow{divide \ 2nd \ by -5} \begin{cases} x+2y+3z=6 \\\ y+2z=4 \\\ -7y-4z=2 \end{cases}$

$\underrightarrow{3rd \ = \ 3rd+7 \times 2nd} \begin{cases} x+2y+3z=6 \\\ y+2z=4 \\\ 10z=30 \end{cases}$

At this point we’ve eliminated both x and y from the third equation, and we can solve $10z=30$
to get $z=3$ .  
Substituting for z in the second equation gives $y+2 \cdot 3 =4$,
or $y=-2$.  

> **Warning**  
> 下面与html页面答案不同，我认为自己的是正确的。

Substituting for y and z in the first equation gives $x+2 \cdot (-2)+3 \cdot 3=6$,
or $x=1$.  
Thus the only solution is (x,y,z)=(1,-2,3).  
We can check that our solution is correct by substituting (x,y,z)=(1,-2,3) into the original equation:  

$$\begin{cases} x+2y+3z=6 \\\ 2x-3y+2z=14 \\\ 3x+y-z=-2 \end{cases} \underrightarrow{substitute} \begin{cases} 1+2 \cdot (-2) +3\cdot 3=6 \\\ 2\cdot 1-3\cdot (-2)+2\cdot 3=14 \\\ 3\cdot 1+\cdot (-2)-3=-2 \end{cases} $$


>Augmented Matrices and Row Operations

Solving equations by elimination requires writing the variables _x,y,z_ and the equals sign = over and over again, merely as placeholders:  
all that is changing in the equations is the coefficient numbers.  
We can make our life easier by extracting only the numbers, and putting them in a box:  

$$\begin{cases} x+2y+3z=6 \\\ 2x-3y+2z=14 \\\ 3x+y-z=-2 \end{cases} \underrightarrow{becomes} \left(\begin{array}{ccc|c} 1 & 2 & 3 & 6 \\\ 2 & -3 & 2 & 14 \\\ 3 & 1 & -1 & -2 \end{array}\right)$$

This is called an augmented matrix.  
The word “augmented” refers to the vertical line, which we draw to remind ourselves where the equals sign belongs;  
a matrix is a grid of numbers without the vertical line.  
In this notation, our three valid ways of manipulating our equations become **row operations**:

...

>Definition  
Two matrices are called **row equivalent** if one can be obtained from the other by doing some number of row operations.  

### Echelon Forms

...

>Definition  
A matrix is in row echelon form if:  
1. All zero rows are at the bottom.  
2. The first nonzero entry of a row is to the right of the first nonzero entry of the row above.  
3. Below the first nonzero entry of a row, all entries are zero.  

...

>Definition  
**A pivot** is the first nonzero entry of a row of a matrix in row echelon form.


>Definition  
A matrix is in **reduced row echelon form** if it is in row echelon form, and in addition:  
4. Each pivot is equal to 1.  
5. Each pivot is the only nonzero entry in its column.  

...

### The Row Reduction Algorithm

...

>Definition  
**A pivot position** of a matrix is an entry that is a pivot of a row echelon form of that matrix.  
**A pivot column** of a matrix is a column that contains a pivot position.  


...


>The Row Echelon Form of an **Inconsistent System**.  
An augmented matrix corresponds to an inconsistent system of equations if and only if the last column (i.e., the augmented column) is a pivot column.

...

## 《Interactive Linear Algebra》_1.3 Parametric Form

### Objectives
* Learn to express the solution set of a system of linear equations in parametric form.  

* Understand the three possibilities for the number of solutions of a system of linear equations.  

* Recipe: parametric form.  

* Vocabulary word:  
  * free variable.

### Free Variables

...

>Recipe: Parametric form  
The **parametric form** of the solution set of a consistent system of linear equations is obtained as follows.  
1. Write the system as an augmented matrix.
2. Row reduce to reduced row echelon form.
3. Write the corresponding (solved) system of linear equations.
4. Move all free variables to the right hand side of the equations.

...

>Implicit Versus Parameterized Equations  
...



# en_math_2022_07-05  

**《Introduction to Linear Algebra, 5th》**

## Preface 
I am happy for you to see this Fifth Edition of Introduction to Linear Algebra.  
This is the text for my video lectures on MIT's OpenCourseWare (ocw.mit.edu and also YouTube).  
I hope those lectures will be useful to you (maybe even enjoyable!).  

Hundreds of colleges and universities have chosen this textbook for their basic linear algebra course.  
A sabbatical gave me a chance to prepare two new chapters about probability and statistics and understanding data.  
Thousands of other improvements too­ --- probably only noticed by the author...  
Here is a new addition for students and all readers:  

>Every section opens with a brief summary to explain its contents.  
>When you read a new section, and when you revisit a section to review and organize it in your mind, those lines are a quick guide and an aid to memory.  

Another big change comes on this book's website math.mit.edu/linearalgebra.  
That site now contains solutions to the Problem Sets in the book.  
With unlimited space, this is much more flexible than printing short solutions.  
There are three key websites:  

[ocw.mit.edu](ocw.mit.edu)  
Messages come from thousands of students and faculty about linear algebra on this OpenCourseWare site.  
The 18.06 and 18.06 SC courses include video lectures of a complete semester of classes.  
Those lectures offer an independent review of the whole subject based on this textbook-the professor's time stays free and the student's time can be 2 a.m. (The reader doesn't have to be in a class at all.)  
Six million viewers around the world have seen these videos (amazing).  
I hope you find them helpful.  


[web.mit.edu/18.06](web.mit.edu/18.06)  
This site has homeworks and exams (with solutions) for the current course as it is taught, and as far back as 1996.  
There are also review questions, Java demos, Teaching Codes, and short essays (and the video lectures).  
My goal is to make this book as useful to you as possible, with all the course material we can provide.  


[math.mit.edu/linearalgebra](math.mit.edu/linearalgebra)  
This has become an active website.  
It now has Solutions to Exercises-with space to explain ideas.  
There are also new exercises from many dif­ferent sources-practice problems, development of textbook examples, codes in MATLAB and Julia and Python, plus whole collections of exams (18.06 and others) for review.  

Please visit this linear algebra site.  
Send suggestions to linearalgebrabook@gmail.com

### The Fifth Edition 
The cover shows the **Four Fundamental Subspaces** - the row space and nullspace are on the left side, the column space and the nulls pace of $A^T$ are on the right.  
It is not usual to put the central ideas of the subject on display like this!  
When you meet those four spaces in Chapter 3, you will understand why that picture is so central to linear algebra.  

Those were named the Four Fundamental Subspaces in my first book, and they start from a matrix A.  
Each row of A is a vector in n-dimensional space.  
When the matrix has m rows, each column is a vector in m-dimensional space.  
The crucial operation in linear algebra is to take **linear combinations of column vectors**.  
This is exactly the result of a matrix-vector multiplication.  
$Ax$ 
is a combination of the columns of $A$.  

When we take all combinations $Ax$ of the column vectors, we get the column space.  
If this space includes the vector $b$
, we can solve the equation $Ax = b$.  

May I call special attention to Section 1.3, where these ideas come early-with two specific examples.  
You are not expected to catch every detail of vector spaces in one day!  
But you will see the first matrices in the book, and a picture of their column spaces.  
There is even an inverse matrix and its connection to calculus.  
You will be learning the language of linear algebra in the best and most efficient way: by using it. 

Every section of the basic course ends with a large collection of review problems.  
They ask you to use the ideas in that section -- the dimension of the column space, a basis for that space, the rank and inverse and determinant and eigenvalues of $A$.  
Many problems look for computations by hand on a small matrix, and they have been highly praised.  
The Challenge Problems go a step further, and sometimes deeper.  

Let me give four examples: 

Section 2.1:  
Which row exchanges of a Sudoku matrix produce another Sudoku matrix? 

Section 2.7:  
If $P$ 
is a permutation matrix, why is some power $p^k$ equal to 
$I$?  

Section 3.4: 
If $Ax=b$ 
and $Cx = b$ 
have the same solutions for every $b$, 
does $A$ equal 
$C$? 

Section 4.1:  
What conditions on the four vectors $r,  n,  c, £$ 
allow them to be bases for the row space, the nullspace, the column space, and the left nullspace of a 2 by 2 matrix?

### The Start of the Course 
The equation $Ax = b$ uses the language of linear combinations right away.  
The vector $Ax$ is a combination of the columns of 
$A$.  
The equation is asking for $a$ combination that produces 
$b$.  
The solution vector $x$ comes at three levels and all are important:  

1. **Direct solution** to find x by forward elimination and back substitution.

2. **Matrix solution** using the inverse matrix: $x = A^{-1}b$ 
(if $A$ has an inverse).

3. **Particular solution** (to $Ay = b$) plus **nullspace solution**
(to $Az = 0$).  

That vector space solution $x = y + z$ is shown on the cover of the book.  

Direct elimination is the most frequently used algorithm in scientific computing.  
The matrix $A$ becomes triangular --- then solutions come quickly.  
We also see bases for the four subspaces.  
But don't spend forever on practicing elimination ... good ideas are coming.  
The speed of every new supercomputer is tested on $Ax = b$ : pure linear algebra.  
But even a supercomputer doesn't want the inverse matrix: too slow.  
Inverses give the simplest formula $x = A^{-1}b$ but not the top speed.  
And everyone must know that determinants are even slower --- there is no way a linear algebra course should begin with formulas for the determinant of an n by n matrix.  
Those formulas have a place, but not first place.  

### Structure of the Textbook 

Already in this preface, you can see the style of the book and its goal.  
That goal is serious, to explain this beautiful and useful part of mathematics.  
You will see how the applications of linear algebra reinforce the key ideas.  
This book moves gradually and steadily from numbers to vectors to subspaces --- each level comes naturally and everyone can get it.  

Here are 12 points about learning and teaching from this book :   

1.
Chapter 1 starts with vectors and dot products.  
If the class has met them before, focus quickly on linear combinations.  
Section 1.3 provides three independent vectors whose combinations fill all of 3-dimensional space, and three dependent vectors in a plane.  
**Those two examples are the beginning of linear algebra.**  


2.
Chapter 2 shows the row picture and the column picture of $Ax = b$.  
The heart of linear algebra is in that connection between the rows of A and the columns of A :  
the same numbers but very different pictures.  
Then begins the algebra of matrices:  
an elimination matrix E multiplies A to produce a zero.  
The goal is to capture the whole process-start with A, multiply by E's, end with U.  

Elimination is seen in the beautiful form $A = LU$.  
The **lower triangular** L holds the forward elimination steps, and U is **upper triangular** for back substitution.  


3.
Chapter 3 is linear algebra at the best level: subspaces.  
The column space contains all linear combinations of the columns.  
The crucial question is:  
**How many of those columns are needed?**  
The answer tells us the dimension of the column space, and the key information about A.  
We reach the Fundamental Theorem of Linear Algebra.  

4.
With more equations than unknowns, it is almost sure that $Ax = b$ has no solution.  
We cannot throw out every measurement that is close but not perfectly exact!  
When we solve by **least squares**, the key will be the matrix $A^T A$.  
This wonderful matrix appears everywhere in applied mathematics, when A is rectangular.  

5.
**Determinants** give formulas for all that has come before-Cramer's Rule, inverse matrices, volumes inn dimensions.  
We don't need those formulas to com­ pute.  
They slow us down.  
But det A = 0 tells when a matrix is singular:  
this is the key to eigenvalues.  

6.
**Section 6.1 explains eigenvalues for 2 by 2 matrices.**  
Many courses want to see eigenvalues early.  
It is completely reasonable to come here directly from Chapter 3, because the determinant is easy for a 2 by 2 matrix.  
The key equation is $Ax= \lambda x$.  
Eigenvalues and eigenvectors are an astonishing way to understand a square matrix.  
They are not for $Ax = b$
, they are for dynamic equations like $du/ dt = Au$.  
The idea is always the same:  
follow the eigenvectors.  
In those special directions, A acts like a single number (the eigenvalue $\lambda$) and the problem is one-dimensional.  
An essential highlight of Chapter 6 is **diagonalizing a symmetric matrix**.  
When all the eigenvalues are positive, the matrix is "positive definite".  
This key idea connects the whole course-positive pivots and determinants and eigenvalues and energy.  
I work hard to reach this point in the book and to explain it by examples.  

7.
Chapter 7 is new.  
It introduces **singular values** and **singular vectors**.  
They separate all martices into simple pieces, ranked in order of their importance.  
You will see one way to compress an image.  
Especially you can analyze a matrix full of data.  

8.
Chapter 8 explains **linear transformations**.  
This is geometry without axes, algebra with no coordinates.  
When we choose a basis, we reach the best possible matrix.  

9.
Chapter 9 moves from real numbers and vectors to complex vectors and matrices.  
The Fourier matrix F is the most important complex matrix we will ever see.  
And the **Fast Fourier Transform** (multiplying quickly by F and p-1) is revolutionary.  

10.
Chapter 10 is full of applications, more than any single course could need:

10.1 Graphs and Networks --- leading to the edge-node matrix for Kirchhoff's Laws  

10.2 Matrices in Engineering --- differential equations parallel to matrix equations  

10.3 Markov Matrices --- as in Google's PageRank algorithm  

10.4 Linear Programming --- a new requirement $x \ge 0$ and minimization of the cost

10.5 Fourier Series --- linear algebra for functions and digital signal processing

10.6 Computer Graphics-matrices move and rotate and compress images

10.7 Linear Algebra in Cryptography-this new section was fun to write.  
The Hill Cipher is not too secure.  
It uses modular arithmetic: integers from $0 \ to \ p-1$. 
Multiplication gives $4 x 5 \equiv	 1 (mod \ 19)$.  
For decoding this gives $4^{-1} = 5$. 

11.
How should computing be included in a linear algebra course?  
It can open a new understanding of matrices-every class will find a balance.  
MATLAB and Maple and Mathematica are powerful in different ways.  
Julia and Python are free and directly accessible on the Web.  
Those newer languages are powerful too !  
Basic commands begin in Chapter 2.  
Then Chapter 11 moves toward professional al­gorithms.  
You can upload and download codes for this course on the website.  

12.
Chapter 12 on Probability and Statistics is new, with truly important applications.  
When random variables are not independent we get covariance matrices.  
Fortunately they are symmetric positive definite.  
The linear algebra in Chapter 6 is needed now.  


### The Variety of Linear Algebra 
Calculus is mostly about one special operation (the derivative) and its inverse (the integral).  
Of course I admit that calculus could be important ...  
But so many applications of math­ ematics are discrete rather than continuous, digital rather than analog.  
The century of data has begun!  
You will find a light-hearted essay called "Too Much Calculus" on my website.  
**The truth is that vectors and matrices have become the language to know.**  

Part of that language is the wonderful variety of matrices.  
Let me give three examples:  

Symmetric matrix ：$\begin{bmatrix} 2 & -1 & 0 & 0 \\\ -1 & 2 & -1 & 0 \\\ 0 & -1 & 2 & -1 \\\ 0 & 0 & -1 & 2 \end{bmatrix}$  

Orthogonal matrix ：$\frac{1}{2} \begin{bmatrix} 1 & 1 & 1 & 1 \\\ -1 & -1 & 1 & -1 \\\ 1 & 1 & -1 & -1 \\\ 1 & -1 & -1 & 1 \end{bmatrix}$  

Triangular matrix ：$\begin{bmatrix} 1 & 1 & 1 & 1 \\\ 0 & 1 & 1 & 1 \\\ 0 & 0 & 1 & 1 \\\ 0 & 0 & 0 & 1 \end{bmatrix}$  


A key goal is learning to "read" a matrix.  
You need to see the meaning in the numbers.  
This is really the essence of mathematics-patterns and their meaning.  

I have used _italics_ and **boldface** to pick out the key words on each page.  
I know there are times when you want to read quickly, looking for the important lines.  

May I end with this thought for professors.  
You might feel that the direction is right, and wonder if your students are ready.  
**Just give them a chance!**  
Literally thousands of students have written to me, frequently with suggestions and surprisingly often with thanks.   
They know this course has a purpose, because the professor and the book are on their side.  
Linear algebra is a fantastic subject, enjoy it.

### Help With This Book  
The greatest encouragement of all is the feeling that you are doing something worthwhile with your life.  
Hundreds of generous readers have sent ideas and examples and corrections (and favorite matrices) that appear in this book.  
Thank you all.  

One person has helped with every word in this book.  
He is Ashley C. Fernandes, who prepared the Latex files.  
It is now six books that he has allowed me to write and rewrite, aiming for accuracy and also for life.  
Working with friends is a happy way to live.  

Friends inside and outside the MIT math department have been wonderful.  
Alan Edelman for Julia and much more, Alex Townsend for the flag examples in 7.1, and Peter Kempthorne for the finance example in 7.3: those stand out.  
Don Spickler's website on cryptography is simply excellent.  
I thank Jon Bloom, Jack Dongarra, Hilary Finucane, Pavel  Grinfeld, Randy LeVeque, David Vogan, Liang Wang, and Karen Willcox.  
The "eigenfaces" in 7.3 came from Matthew Turk and Jeff Jauregui.  
And the big step to singular values was accelerated by Raj Rao's great course at Michigan.  
This book owes so much to my happy sabbatical in Oxford.  
Thank you, Nick Trefethen and everyone.  
Especially you the reader!  
Best wishes in your work.  

### Background of the Author  
This is my 9th textbook on linear algebra, and I hesitate to write about myself.  
It is the mathematics that is important, and the reader.  
The next paragraphs add something brief and personal, as a way to say that textbooks are written by people.  
I was born in Chicago and went to school in Washington and Cincinnati and St. Louis.  
My college was MIT (and my linear algebra course was extremely abstract).   
After that came Oxford and UCLA, then back to MIT for a very long time.  
I don't know how many thousands of students have taken 18.06 (more than 6 million when you include the videos on ocw.mit.edu).  
The time for a fresh approach was right, because this fantastic subject was only revealed to math majors --- **we needed to open linear algebra to the world**.  
I am so grateful for a life of teaching mathematics, more than I could possibly tell you.  

--- Gilbert Strang 

PS  
I hope the next book (2018?) will include _Learning from Data_.  
This subject is grow­ing quickly, especially "deep learning".  
By knowing a function on a training set of old data, we approximate the function on new data.  
The approximation only uses one simple non­-linear function $f(x) = max(0, x)$.  
It is n matrix multiplications that we optimize to make the learning deep:  
$x_1 = f(A_1x + b_1)$
, $x_2 = f(A_2x_1 + b_2), ... , x_n = f(A_nX_{n-1} + b_n)­$.  
Those are $(n -1)$ hidden layers between the input x and the output 
$X_n$ ---which approximates  
$F(x)$ on the training set.

### THE MATRIX ALPHABET  
A---Any Matrix  

B---Basis Matrix  

C---Cofactor Matrix  

D---Diagonal Matrix  

E---Elimination Matrix  

F---Fourier Matrix  

H---Hadamard Matrix  

I---Identity Matrix  

K---Stiffness Matrix  

L---Lower Triangular Matrix  

M---Markov Matrix  

N---Nullspace Matrix  

P---Permutation Matrix  

P---Projection Matrix  

Q---Orthogonal Matrix  

R---Upper Triangular Matrix  

S---Reduced Echelon Matrix  

T---Symmetric Matrix  

U---Linear Transformation  

V---Upper Triangular Matrix  

W---Left Singular Vectors  

X---Right Singular Vectors  

Y---Eigenvector Matrix  

$\Lambda$---Eigenvalue Matrix  

$\Sigma$---Singular Value Matrix



# en_math_2022_07-03

## Mit18.06_LinearAlgebra_LECTURE_1

**Video Lectures**

A major application of linear algebra is to solving systems of linear equations.  
This lecture presents three ways of thinking about these systems.  
The “row method” focuses on the individual equations, the “column method” focuses on combining the columns, and the “matrix method” is an even more compact and powerful way of describing systems of linear equations.  

These video lectures of Professor Gilbert Strang teaching 18.06 were recorded in Fall 1999 and do not correspond precisely to the current edition of the textbook.  
However, this book is still the best reference for more information on the topics covered in each lecture.  


Reading assignments are also provided for the newer edition:  
Strang, Gilbert. Introduction to Linear Algebra. 5th ed.
[Wellesley-Cambridge Press](http://www.wellesleycambridge.com/), 2016. ISBN: 9780980232776


**Instructor/speaker**: Prof. Gilbert Strang


### video_1-The-Geometry-of-Linear-Equations

>**Note**  
>此处有视频，视频本身我不贴上来，自己去原网页看；但我把字幕放在下面 

Hi.  
This is the first lecture in MIT's course 18.06, linear algebra, and I'm Gilbert Strang.  
The text for the course is this book, Introduction to Linear Algebra.  
And the course web page, which has got a lot of exercises from the past, MatLab codes, the syllabus for the course, is web.mit.edu/18.06.  
And this is the first lecture, lecture one.  
So, and later we'll give the web address for viewing these, videotapes.  
Okay, so what's in the first lecture?  
This is my plan.  
The fundamental problem of linear algebra, which is to solve a system of linear equations.  
So let's start with a case when we have some number of equations, say n equations and n unknowns.  
So an equal number of equations and unknowns.  
That's the normal, nice case.  


And what I want to do is -- with examples, of course -- to describe, first, what I call the Row picture.  
That's the picture of one equation at a time.  
It's the picture you've seen before in two by two equations where lines meet.  
So in a minute, you'll see lines meeting.  


The second picture, I'll put a star beside that, because that's such an important one.  
And maybe new to you is the picture -- a column at a time.  
And those are the rows and columns of a matrix.  


So the third -- the algebra way to look at the problem is the matrix form and using a matrix that I'll call A.  


Okay, so can I do an example?  
The whole semester will be examples and then see what's going on with the example.  

So, take an example.  
Two equations, two unknowns.  
So let me take 2x -y =0, let's say.  
And -x 2y=3.  
Okay.  
let me -- I can even say right away --
what's the matrix, that is, what's the coefficient matrix?  
The matrix that involves these numbers -- a matrix is just a rectangular array of numbers.  
Here it's two rows and two columns, so 2 and -- minus 1 in the first row minus 1 and 2 in the second row, that's the matrix.  
And the right-hand -- the, unknown -- well, we've got two unknowns.  
So we've got a vector, with two components, x and y, and we've got two right-hand sides that go into a vector 0 3.  


I couldn't resist writing the matrix form, right -- even before the pictures.  
So I always will think of this as the matrix A, the matrix of coefficients, then there's a vector of unknowns.  
Here we've only got two unknowns.  
Later we'll have any number of unknowns.  
And that vector of unknowns, well I'll often -- I'll make that x -- extra bold. 

A and the right-hand side is also a vector that I'll always call b.  

So linear equations are A x equal b and the idea now is to solve this particular example and then step back to see the bigger picture.  


Okay, what's the picture for this example, the Row picture?  
Okay, so here comes the Row picture.
So that means I take one row at a time and I'm drawing here the xy plane and I'm going to plot all the points that satisfy that first equation.  
So I'm looking at all the points that satisfy 2x-y =0.  

It's often good to start with which point on the horizontal line -- on this horizontal line, y is zero.  
The x axis has y as zero and that -- in this case, actually, then x is zero.  
So the point, the origin -- the point with coordinates (0,0) is on the line.  
It solves that equation.  
Okay, tell me in -- well, I guess I have to tell you another point that solves this same equation.  
Let me suppose x is one, so I'll take x to be one.  
Then y should be two, right?  
So there's the point one two that also solves this equation.  

And I could put in more points.  
But, but let me put in all the points at once, because they all lie on a straight line.  

This is a linear equation and that word linear got the letters Okay, thanks.  
for line in it.  
That's the equation -- this is the line that ... of solutions to 2x-y=0 my first row, first equation.  

So typically, maybe, x equal a half, y equal one will work.  
And sure enough it does.  
Okay, that's the first one.  



Now the second one is not going to go through the origin.  
It's always important.  
Do we go through the origin or not?  
In this case, yes, because there's a zero over there.  
In this case we don't go through the origin, because if x and y are zero, we don't get three.  
So, let me again say suppose y is zero, what x do we actually get?  
If y is zero, then I get x is minus three.  
So if y is zero, I go along minus three.  

So there's one point on this second line.  
Now let me say, well, suppose x is minus one -- just to take another x.  
If x is minus one, then this is a one and I think y should be a one, because if x is minus one, then I think y should be a one and we'll get that point.  
Is that right?  
If x is minus one, that's a one.  
If y is a one, that's a two and the one and the two make three and that point's on the equation.  
Okay.  



Now, I should just draw the line, right, connecting those two points at -- that will give me the whole line.  
And if I've done this reasonably well, I think it's going to happen to go through -- well, not happen -- it was arranged to go through that point.  
So I think that the second line is this one, and this is the all-important point that lies on both lines.  

Shall we just check that that point which is the point x equal one and y was two, right?  
That's the point there and that, I believe, solves both equations.  
Let's just check this.  
If x is one, I have a minus one plus four equals three, okay.  
Apologies for drawing this picture that you've seen before.  
But this -- seeing the row picture -- first of all, for n equal 2, two equations and two unknowns, it's the right place to start.  
Okay.  
So we've got the solution.  
The point that lies on both lines.  



Now can I come to the column picture?  
Pay attention, this is the key point.  
So the column picture.  

I'm now going to look at the columns of the matrix.  
I'm going to look at this part and this part.  

I'm going to say that the x part is really x times -- you see, I'm putting the two -- I'm kind of getting the two equations at once -- that part and then I have a y and in the first equation it's multiplying a minus one and in the second equation a two, and on the right-hand side, zero and three.  
You see, the columns of the matrix, the columns of A are here and the right-hand side b is there.  
And now what is the equation asking for?  

It's asking us to find -- somehow to combine that vector and this one in the right amounts to get that one.  
It's asking us to find the right linear combination -- this is called a linear combination.  

And it's the most fundamental operation in the whole course.  
It's a **linear combination of the columns**.  

That's what we're seeing on the left side.  
Again, I don't want to write down a big definition.  
You can see what it is.  

There's column one, there's column two.  
I multiply by some numbers and I add.  
That's a combination -- a linear combination and I want to make those numbers the right numbers to produce zero three.  



Okay.  
Now I want to draw a picture that, represents what this -- this is algebra.  
What's the geometry, what's the picture that goes with it?  
Okay.  
So again, these vectors have two components, so I better draw a picture like that.  
So can I put down these columns?  
I'll draw these columns as they are, and then I'll do a combination of them.  
So the first column is over two and down one, right?  
So there's the first column.  
The first column.  
Column one.  
It's the vector two minus one.  
The second column is -- minus one is the first component and up two.  
It's here.
There's column two.  
So this, again, you see what its components are.  
Its components are minus one, two. Good.  
That's this guy.  



Now I have to take a combination.  
What combination shall I take?  
Why not the right combination, what the hell?  
Okay.  
So the combination I'm going to take is the right one to produce zero three and then we'll see it happen in the picture.  
So the right combination is to take x as one of those and two of these.  
It's because we already know that that's the right x and y, so why not take the correct combination here and see it happen?  
Okay, so how do I picture this linear combination?  

So I start with this vector that's already here -- so that's one of column one, that's one times column one, right there.  
And now I want to add on -- so I'm going to hook the next vector onto the front of the arrow will start the next vector and it will go this way.  
So let's see, can I do it right?  

If I added on one of these vectors, it would go left one and up two, so we'd go left one and up two, so it would probably get us to there.  
Maybe I'll do dotted line for that.  
Okay?  

That's one of column two tucked onto the end, but I wanted to tuck on two of column two.  
So that -- the second one -- we'll go up left one and up two also.  
It'll probably end there.  
And there's another one.  
So what I've put in here is two of column two.  
Added on.  
And where did I end up?  
What are the coordinates of this result?  

What do I get when I take one of this plus two of that?  
I do get that, of course.  
There it is, x is zero, y is three, that's b.  
That's the answer we wanted.  

And how do I do it?  
You see I do it just like the first component.  
I have a two and a minus two that produces a zero, and in the second component I have a minus one and a four, they combine to give the three.  
But look at this picture.  
So here's our key picture.  
I combine this column and this column to get this guy.  
That was the b.  
That's the zero three.  
Okay.  

So that idea of linear combination is crucial, and also -- do we want to think about this question?  
Sure, why not.  
What are all the combinations?  
If I took -- can I go back to xs and ys?  
This is a question for really -- it's going to come up over and over, but why don't we see it once now?  
If I took all the xs and all the ys, all the combinations, what would be all the results?  

And, actually, the result would be that I could get any right-hand side at all.  
The combinations of this and this would fill the whole plane.  
You can tuck that away.  
We'll, explore it further.  
But this idea of what linear combination gives b and what do all the linear combinations give, what are all the possible, achievable right-hand sides be -- that's going to be basic.  
Okay.  

Can I move to three equations and three unknowns?  
Because it's easy to picture the two by two case.  
Let me do a three by three example.  

Okay, I'll sort of start it the same way, say maybe 2x-y and maybe I'll take no zs as a zero and maybe a -x 2y and maybe a -z as a -- oh, let me make that a minus one and, just for variety let me
take, -3z, -3ys, I should keep the ys in that line, and 4zs is, say, 4. Okay.  
That's three equations.  
I'm in three dimensions, x, y, z.  
And, I don't have a solution yet.  
So I want to understand the equations and then solve them.  
Okay.  

So how do I you understand them?  
The row picture one way.  
The column picture is another very important way.  

Just let's remember the matrix form, here, because that's easy.  
The matrix form -- what's our matrix A?  
Our matrix A is this right-hand side, the two and the minus one and the zero from the first row, the minus one and the two and the minus one from the second row, the zero, the minus three and the four from the third row.  
So it's a three by three matrix.  
Three equations, three unknowns.  
And what's our right-hand side?  
Of course, it's the vector, zero minus one, four.  
Okay.  
So that's the way, well, that's the short-hand to write out the three equations.  

But it's the picture that I'm looking for today.  
Okay, so the row picture.  
All right, so I'm in three dimensions, x, find out when there isn't a solution.  
y and z.  

And I want to take those equations one at a time and ask -- and make a picture of all the points that satisfy -- let's take equation number two.  
If I make a picture of all the points that satisfy -- all the x, y, z points that solve this equation -- well, first of all, the origin is not one of them.  
x, y, z -- it being 0, 0, 0 would not solve that equation.  
So what are some points that do solve the equation?  
Let's see, maybe if x is one, y and z could be zero.  
That would work, right?  
So there's one point.  
I'm looking at this second equation, here, just, to start with.  
Let's see.  
Also, I guess, if z could be one, x and y could be zero, so that would just go straight up that axis.  
And, probably I'd want a third point here.  
Let me take x to be zero, z to be zero, then y would be minus a half, right?  
So there's a third point, somewhere -- oh my -- okay.  
Let's see.  
I want to put in all the points that satisfy that equation.  

Do you know what that bunch of points will be?  
It's a plane.  
If we have a linear equation, then, fortunately, the graph of the thing, the plot of all the points that solve it are a plane.  
These three points determine a plane, but your lecturer is not Rembrandt and the art is going to be the weak point here.  
So I'm just going to draw a plane, right?  
There's a plane somewhere.  
That's my plane.  
That plane is all the points that solves this guy.  

Then, what about this one?  
Two x minus y plus zero z.  
So z actually can be anything.  
Again, it's going to be another plane.  
Each row in a three by three problem gives us a plane in three dimensions.  
So this one is going to be some other plane -- maybe I'll try to draw it like this.  

And those two planes meet in a line.  
So if I have two equations, just the first two equations in three dimensions, those give me a line.  
The line where those two planes meet.  

And now, the third guy is a third plane.  
And it goes somewhere.  
Okay, those three things meet in a point.  
Now I don't know where that point is, frankly.  
But -- linear algebra will find it.  
The main point is that the three planes, because they're not parallel, they're not special.  
They do meet in one point and that's the solution.  

But, maybe you can see that this row picture is getting a little hard to see.  
The row picture was a cinch when we looked at two lines meeting.  
When we look at three planes meeting, it's not so clear and in four dimensions probably a little less clear.  



So, can I quit on the row picture?  
Or quit on the row picture before I've successfully found the point where the three planes meet?  
All I really want to see is that the row picture consists of three planes and, if everything works right, three planes meet in one point and that's a solution.  

Now, you can tell I prefer the column picture.  
Okay, so let me take the column picture.  

That's x times -- so there were two xs in the first equation minus one x is, and no xs in the third.
It's just the first column of that.  
And how many ys are there?  
There's minus one in the first equations, two in the second and maybe minus three in the third.  
Just the second column of my matrix.  
And z times no zs minus one zs and four zs.  
And it's those three columns, right, that I have to combine to produce the right-hand side, which is zero minus one four.  
Okay.  

So what have we got on this left-hand side?  
A linear combination.  
It's a linear combination now of three vectors, and they happen to be -- each one is a three dimensional vector, so we want to know what combination of those three vectors produces that one.  
Shall I try to draw the column picture, then?  

So, since these vectors have three components -- so it's some multiple -- let me draw in the first column as before -- x is two and y is minus one.  
Maybe there is the first column.  

y -- the second column has maybe a minus one and a two and the y is a minus three, somewhere, there possibly, column two.  

And the third column has -- no zero minus one four, so how shall I draw that?  
So this was the first component.  
The second component was a minus one.  
Maybe up here.  
That's column three, that's the column zero minus one and four.  
This guy.  

So, again, what's my problem?  
What this equation is asking me to do is to combine these three vectors with a right combination to produce this one.  
Well, you can see what the right combination is, because in this special problem, specially chosen by the lecturer, that right-hand side that I'm trying to get is actually one of these columns.  
So I know how to get that one.  

So what's the solution?  
What combination will work?  
I just want one of these and none of these.  
So x should be zero, y should be zero and z should be one.  
That's the combination.  
One of those is obviously the right one.  
Column three is actually the same as b in this particular problem.  

I made it work that way just so we would get an answer, (0,0,1), so somehow that's the point where those three planes met and I couldn't see it before.  
Of course, I won't always be able to see it from the column picture, either.  



It's the next lecture, actually, which is about elimination, which is the systematic way that everybody -- every bit of software, too -- production, large-scale software would solve the equations.  
So the lecture that's coming up.  
If I was to add that to the syllabus, will be about how to find x, y, z in all cases.  
Can I just think again, though, about the big picture?  

By the big picture I mean let's keep this same matrix on the left but imagine that we have a different right-hand side.  
Oh, let me take a different right-hand side.  
So I'll change that right-hand side to something that actually is also pretty special.  
Let me change it to -- if I add those first two columns, that would give me a one and a one and a minus three.  
There's a very special right-hand side.  
I just cooked it up by adding this one to this one.  
Now, what's the solution with this new right-hand side?  
The solution with this new right-hand side is clear.  
took one of these and none of those.  
So actually, it just changed around to this when I took this new right-hand side.  
Okay.  
So in the row picture, I have three different planes, three new planes meeting now at this point.  
In the column picture, I have the same three columns, but now I'm combining them to produce this guy, and it turned out that column one plus column two which would be somewhere -- there is the right column -- one of this and one of this would give me the new b.  
Okay.  
So we squeezed in an extra example.  

But now think about all bs, all right-hand sides.  
Can I solve these equations for every right-hand side?  
Can I ask that question?  
So that's the algebra question.  
Can I solve Ax=b for every b?  
Let me write that down.  
Can I solve Ax=b for every right-hand side b?  
I mean, is there a solution?  
And then, if there is, elimination will give me a way to find it.  
I really wanted to ask, is there a solution for every right-hand side?  
So now, can I put that in different words -- in this linear combination words?  
So in linear combination words, do the linear combinations of the columns fill three dimensional space?  
Every b means all the bs in three dimensional space.  

Do you see that I'm just asking the same question in different words?  
Solving A x -- A x -- that's very important.  
A times x -- when I multiply a matrix by a vector, I get a combination of the columns.  
I'll write that down in a moment.  
But in my column picture, that's really what I'm doing.  
I'm taking linear combinations of these three columns and I'm trying to find b.  
And, actually, the answer for this matrix will be yes.  
For this matrix A -- for these columns, the answer is yes.  
This matrix -- that I chose for an example is a good matrix.  
A non-singular matrix.  
An invertible matrix.  
Those will be the matrices that we like best.  
There could be other -- and we will see other matrices where the answer becomes, no --
oh, actually, you can see when it would become no.  
What could go wrong?  
find out -- because if elimination fails, How could it go wrong that out of these -- out of three columns and all their combinations -- when would I not be able to produce some b off here?  
When could it go wrong?  
Do you see that the combinations -- let me say when it goes wrong.  
If these three columns all lie in the same plane, then their combinations will lie in that same plane.  
So then we're in trouble.  

If the three columns of my matrix -- if those three vectors happen to lie in the same plane -- for example, if column three is just the sum of column one and column two, I would be in trouble.  
That would be a matrix A where the answer would be no, because the combinations -- if column three is in the same plane as column one and two, I don't get anything new from that.  
All the combinations are in the plane and only right-hand sides b that I could get would be the ones in that plane.  
So I could solve it for some right-hand sides, when b is in the plane, but most right-hand sides would be out of the plane and unreachable.  
So that would be a singular case.  
The matrix would be not invertible.  
There would not be a solution for every b.  
The answer would become no for that.  
Okay.  



I don't know -- shall we take just a little shot at thinking about nine dimensions?  
Imagine that we have vectors with nine components.  
Well, it's going to be hard to visualize those.  
I don't pretend to do it.  
But somehow, pretend you do.  
Pretend we have -- if this was nine equations and nine unknowns, then we would have nine columns, and each one would be a vector in nine-dimensional space and we would be looking at their linear combinations.  
So we would be having the linear combinations of nine vectors in nine-dimensional space, and we would be trying to find the combination that hit the correct right-hand side b.  
And we might also ask the question can we always do it?  
Can we get every right-hand side b?  
And certainly it will depend on those nine columns.  
Sometimes the answer will be yes -- if I picked a random matrix, it would be yes, actually.  
If I used MatLab and just used the random command, picked out a nine by nine matrix, I guarantee it would be good.  
It would be non-singular, it would be invertible, all beautiful.  
But if I choose those columns so that they're not independent, so that the ninth column is the same as the eighth column, then it contributes nothing new and there would be right-hand sides b that I couldn't get.  
Can you sort of think about nine vectors in nine-dimensional space an take their combinations?  
That's really the central thought -- that you get kind of used to in linear algebra.  
Even though you can't really visualize it, you sort of think you can after a while.  
Those nine columns and all their combinations may very well fill out the whole nine-dimensional space.  
But if the ninth column happened to be the same as the eighth column and gave nothing new, then probably what it would fill out would be -- I hesitate even to say this -- it would be a sort of a plane -- an eight dimensional plane inside nine-dimensional space.  
And it's those eight dimensional planes inside nine-dimensional space that we have to work with eventually.  
For now, let's stay with a nice case where the matrices work, we can get every right-hand side b and here we see how to do it with columns.  
Okay.  

There was one step which I realized I was saying in words that I now want to write in letters.  
Because I'm coming back to the matrix form of the equation, so let me write it here.  
The matrix form of my equation, of my system is some matrix A times some vector x equals some right-hand side b.  
Okay.  
So this is a multiplication.  
A times x.  
Matrix times vector, and I just want to say how do you multiply a matrix by a vector?  

Okay, so I'm just going to create a matrix -- let me take two five one three -- and let me take a vector x to be, say, 1and 2.  
How do I multiply a matrix by a vector?  
But just think a little bit about matrix notation and how to do that in multiplication.  
So let me say how I multiply a matrix by a vector.  
Actually, there are two ways to do it.  
Let me tell you my favorite way.  
It's columns again. 
It's a column at a time.  

For me, this matrix multiplication says I take one of that column and two of that column and add.  
So this is the way I would think of it is one of the first column and two of the second column and let's just see what we get.  
So in the first component I'm getting a two and a ten.  
I'm getting a twelve there. In the second component I'm getting a one and a six, I'm getting a seven.  
So that matrix times that vector is twelve seven.  

Now, you could do that another way.  
You could do it a row at a time.  
And you would get this twelve -- and actually I pretty much did it here -- this way.  
Two -- I could take that row times my vector.  
This is the idea of a **dot product**.  
This vector times this vector, two times one plus five times two is the twelve.  
This vector times this vector -- one times one plus three times two is the seven.  
So I can do it by rows, and in each row times my x is what I'll later call a dot product.  
But I also like to see it by columns.  
I see this as a linear combination of a column.  

So here's my point.  
A times x is a combination of the columns of A.  
That's how I hope you will think of A times x when we need it.  
Right now we've got -- with small ones, we can always do it in different ways, but later, think of it that way.  

Okay.  
So that's the picture for a two by two system.  
And if the right-hand side B happened to be twelve seven, then of course the correct solution would be one two.  
Okay.  
So let me come back next time to a systematic way, using elimination, to find the solution, if there is one, to a system of any size and find out-- because the system-- because if elimination fails-- find out when there isn't a solution.  
Okay.  
Thanks.  

